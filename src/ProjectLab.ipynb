{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/k7xkpa8lwkjanf79r0fj3bhviv4vqm8q-python3-3.10.13-env/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/nix/store/k7xkpa8lwkjanf79r0fj3bhviv4vqm8q-python3-3.10.13-env/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import torch\n",
    "import train\n",
    "\n",
    "# This is example, modify the directory based on the set up.\n",
    "src = \"D:/599DL4VProject/the_wildfire_dataset\"\n",
    "\n",
    "wf1TrainPath = pathlib.Path(src + '/train').resolve()\n",
    "wf1ValidPath = pathlib.Path(src + '/valid').resolve()\n",
    "wf1TestPath = pathlib.Path(src + '/test').resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Idea: What if we train and evaluate using batch of very small random crops (64x64) for each image then adjust loss via bayesian or expected value? Actual set of feature needed to classify a sample can be surprisingly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "tfset = v2.Compose([\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(torch.uint8),\n",
    "        #v2.RandomRotation(degrees=(0,90)),\n",
    "        v2.RandomCrop(size=512, pad_if_needed=True),\n",
    "        v2.RandomApply(transforms=[\n",
    "            v2.RandomInvert(p),\n",
    "            v2.RandomGrayscale(p),\n",
    "            v2.ColorJitter(p,p,p,p),\n",
    "            v2.RandomEqualize(p),\n",
    "            v2.RandomHorizontalFlip(p),\n",
    "        ], p=0.6),\n",
    "        v2.Resize(size=128, antialias=True),\n",
    "        v2.ConvertImageDtype(dtype=torch.float),\n",
    "        v2.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "wf1Train = train.ForestFireDataset(root=str(wf1TrainPath), transform=tfset)\n",
    "wf1Valid = train.ForestFireDataset(root=str(wf1ValidPath), transform=tfset)\n",
    "wf1Test = train.ForestFireDataset(root=str(wf1TestPath), transform=tfset)\n",
    "\n",
    "#wf1fire = Subset(wf1Train, [i for i, label in enumerate(wf1Train.targets) if label == 1])\n",
    "wf1nofire = Subset(wf1Train, [i for i, label in enumerate(wf1Train.targets) if label == 0])\n",
    "\n",
    "batch_size = 16\n",
    "wf1TrLoader = DataLoader(dataset=wf1nofire, batch_size=batch_size, shuffle=True)\n",
    "wf1VaLoader = DataLoader(dataset=wf1Valid, batch_size=batch_size)\n",
    "wf1TsLoader = DataLoader(dataset=wf1Test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "print(\"Training Set: \", Counter(wf1Train.targets))\n",
    "print(\"Valid Set: \", Counter(wf1Valid.targets))\n",
    "print(\"Test Set: \", Counter(wf1Test.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Time: 847.25s\n",
      "Train:  {'rec_loss': 0.6942729375133776, 'gen_loss': 0.4967227047436858, 'd_loss': 1.523321616812928, 'feature_loss': 4.121257290774829}\n",
      "Validation:  {'rec_loss': 3.032583056397699, 'gen_loss': 0.45488234183088466, 'd_loss': 1.6158791916880442}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "iter = 1\n",
    "trainer = train.GANProject(img_size=128, debug=False, small=True)\n",
    "trainer.train_model(trainLoader=wf1TrLoader, validLoader=wf1VaLoader, metric='loss', epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_epoch = 9\n",
    "gen_path = pathlib.Path(f'genproject_e{t_epoch}.pt').resolve()\n",
    "dsc_path = pathlib.Path(f'dscproject_e{t_epoch}.pt').resolve()\n",
    "gen = torch.load(gen_path, map_location=device)\n",
    "dsc = torch.load(dsc_path, map_location=device)\n",
    "predictor = train.GANProject(load_unet=gen, load_cnn=dsc, img_size=128, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = predictor.ganEval_epoch(wf1TsLoader)\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
