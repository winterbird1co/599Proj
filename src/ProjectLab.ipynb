{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import train\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "src = \"\"\n",
    "if os.name == \"nt\":\n",
    "    src = \"D:/599DL4VProject/the_wildfire_dataset\"\n",
    "elif os.name == \"posix\":\n",
    "    src = \"/home/asromelo/Desktop/Projects/599_proj/the_wildfire_dataset/\"\n",
    "\n",
    "wf1TrainPath = pathlib.Path(src + '/train').resolve()\n",
    "wf1ValidPath = pathlib.Path(src + '/valid').resolve()\n",
    "wf1TestPath = pathlib.Path(src + '/test').resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Idea: What if we train and evaluate using batch of very small random crops (64x64) for each image then adjust loss via bayesian or expected value? Actual set of feature needed to classify a sample can be surprisingly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "tfset = v2.Compose([\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(torch.uint8),\n",
    "        v2.Resize(size=1080, max_size=2160, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
    "        v2.RandomCrop(size=512, pad_if_needed=True),\n",
    "        v2.RandomChoice(transforms=[\n",
    "            v2.RandomInvert(p),\n",
    "            v2.ColorJitter(p,p,p,p),\n",
    "            v2.RandomEqualize(p),\n",
    "            v2.RandomHorizontalFlip(p),\n",
    "        ], p=[0.2,0.2,0.2,0.2]),\n",
    "        v2.Resize(size=128, antialias=False),\n",
    "        v2.ConvertImageDtype(dtype=torch.float),\n",
    "        #v2.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "tfset.to(device)\n",
    "\n",
    "testset = v2.Compose([\n",
    "    v2.ToImageTensor(),\n",
    "    v2.ConvertImageDtype(torch.uint8),\n",
    "    v2.Resize(size=1080, max_size=2160, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
    "    v2.CenterCrop(size=512),\n",
    "    v2.Resize(size=128, antialias=False),\n",
    "    v2.ConvertImageDtype(dtype=torch.float)\n",
    "]).to(device)\n",
    "\n",
    "wf1Train = train.ForestFireDataset(root=str(wf1TrainPath), transform=tfset)\n",
    "wf1Valid = train.ForestFireDataset(root=str(wf1ValidPath), transform=testset)\n",
    "wf1Test = train.ForestFireDataset(root=str(wf1TestPath), transform=testset)\n",
    "\n",
    "wf1fire = Subset(wf1Test, [i for i, label in enumerate(wf1Test.targets) if label == 1])\n",
    "wf1nofire = Subset(wf1Train, [i for i, label in enumerate(wf1Train.targets) if label == 0])\n",
    "\n",
    "batch_size = 32\n",
    "wf1TrLoader = DataLoader(dataset=wf1nofire, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "wf1VaLoader = DataLoader(dataset=wf1Valid, batch_size=batch_size, num_workers=2)\n",
    "wf1TsLoader = DataLoader(dataset=wf1Test, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "trainer = train.GANProject(img_size=128, debug=False, small=True, activation=nn.LeakyReLU(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  Counter({1: 1157, 0: 730})\n",
      "Valid Set:  Counter({1: 246, 0: 156})\n",
      "Test Set:  Counter({1: 251, 0: 159})\n",
      "12992900\n",
      "451.15754318237305\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "print(\"Training Set: \", Counter(wf1Train.targets))\n",
    "print(\"Valid Set: \", Counter(wf1Valid.targets))\n",
    "print(\"Test Set: \", Counter(wf1Test.targets))\n",
    "\n",
    "acts = []\n",
    "for name, module in trainer.named_modules():\n",
    "    if name == 'classifier' or name == 'features':\n",
    "        continue\n",
    "    module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
    "\n",
    "X, y_true = next(iter(wf1TrLoader))\n",
    "a,b,c = trainer.singleton(X)\n",
    "\n",
    "model_param_size = sum([p.nelement() for p in trainer.parameters()])\n",
    "grad_size = model_param_size\n",
    "print(model_param_size)\n",
    "batch_mem = batch_size * 3 * 128 * 128\n",
    "opt_size = sum([p.nelement() for p in trainer.opt_Gen.param_groups[0]['params']]) + sum([p.nelement() for p in trainer.opt_Dsc.param_groups[0]['params']]) + sum([p.nelement() for p in trainer.opt_Enc.param_groups[0]['params']])\n",
    "act_size = sum([a.nelement() for a in acts])\n",
    "\n",
    "total_elements = model_param_size + grad_size + batch_mem + opt_size + act_size\n",
    "conv_mb = total_elements * 4 / 1024**2\n",
    "print(conv_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Time: 399.27s\n",
      "Train:  {'rec_loss': 0.08473903185700717, 'd_loss': -0.552120010166952, 'ec_loss': 0, 'feature_loss': 1324.9366438356165}\n",
      "Validation:  {'rec_loss': 0.05728999655045087, 'd_loss': -0.05836157063346597, 'ec_loss': 21873.116915422885, 'accuracy': 0.0009653226504871502}\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                         model_training         2.19%        8.763s        99.80%      399.388s      399.388s        7.936s         2.10%      370.451s      370.451s         352 b    -212.28 Mb     138.58 Mb    -173.42 Gb             1  \n",
      "                                            aten::empty         0.01%      32.068ms         0.01%      32.068ms       3.743us      11.582ms         0.00%      11.582ms       1.352us      11.89 Kb      11.89 Kb      66.51 Gb      66.51 Gb          8567  \n",
      "                                          aten::random_         0.00%      50.000us         0.00%      50.000us      16.667us       4.000us         0.00%       4.000us       1.333us           0 b           0 b           0 b           0 b             3  \n",
      "                                             aten::item         0.01%      22.598ms         0.01%      25.473ms       6.343us       9.718ms         0.00%       25.316s       6.304ms           0 b           0 b           0 b           0 b          4016  \n",
      "                              aten::_local_scalar_dense         0.00%       2.873ms         0.00%       2.873ms       0.723us       25.306s         6.69%       25.306s       6.371ms           0 b           0 b           0 b           0 b          3972  \n",
      "                                         aten::randperm         0.00%     128.000us         0.00%     261.000us      65.250us      16.000us         0.00%      43.000us      10.750us      11.41 Kb         -16 b           0 b           0 b             4  \n",
      "                                    aten::scalar_tensor         0.00%      73.000us         0.00%      73.000us       2.607us      44.000us         0.00%      44.000us       1.571us         208 b         208 b           0 b           0 b            28  \n",
      "                                          aten::resize_         0.00%     213.000us         0.00%     213.000us       1.490us     167.000us         0.00%     167.000us       1.168us           0 b           0 b           0 b           0 b           143  \n",
      "                                     aten::resolve_conj         0.00%       4.000us         0.00%       4.000us       2.000us       2.000us         0.00%       2.000us       1.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                      aten::resolve_neg         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       1.000us           0 b           0 b           0 b           0 b             2  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        96.33%      385.519s        96.33%      385.520s       10.145s     512.000us         0.00%       1.130ms      29.737us     212.26 Mb     212.26 Mb           0 b           0 b            38  \n",
      "                                               aten::to         0.00%      13.061ms         0.71%        2.861s       1.235ms       4.911ms         0.00%      140.010s      60.453ms      15.83 Kb           0 b     216.21 Mb           0 b          2316  \n",
      "                                       aten::lift_fresh         0.00%       2.000us         0.00%       2.000us       0.012us     212.000us         0.00%     212.000us       1.225us           0 b           0 b           0 b           0 b           173  \n",
      "                                          aten::detach_         0.00%     794.000us         0.00%     988.000us       5.646us     415.000us         0.00%     613.000us       3.503us           0 b           0 b           0 b           0 b           175  \n",
      "                                                detach_         0.00%     194.000us         0.00%     194.000us       1.198us     198.000us         0.00%     198.000us       1.222us           0 b           0 b           0 b           0 b           162  \n",
      "                                             aten::set_         0.00%       3.709ms         0.00%       3.709ms       9.584us     352.000us         0.00%     352.000us       0.910us         -16 b         -16 b           0 b           0 b           387  \n",
      "                                       aten::randn_like         0.00%     432.000us         0.00%       1.838ms      79.913us      78.000us         0.00%       1.494ms      64.957us           0 b           0 b     138.64 Mb           0 b            23  \n",
      "                                       aten::empty_like         0.00%      12.457ms         0.01%      23.950ms      15.053us       3.902ms         0.00%       5.659ms       3.557us           0 b           0 b      41.46 Gb           0 b          1591  \n",
      "                                    aten::empty_strided         0.00%      19.923ms         0.00%      19.923ms       3.098us       7.921ms         0.00%       7.921ms       1.232us      15.01 Kb      15.01 Kb       1.99 Gb       1.99 Gb          6430  \n",
      "                                          aten::normal_         0.00%     767.000us         0.00%     767.000us      33.348us       1.329ms         0.00%       1.329ms      57.783us           0 b           0 b           0 b           0 b            23  \n",
      "                                              aten::mul         0.00%       4.523ms         0.00%       4.523ms      22.615us       9.516ms         0.00%       9.516ms      47.580us           0 b           0 b     136.96 Mb     136.96 Mb           200  \n",
      "                                         aten::_to_copy         0.01%      30.094ms         0.71%        2.848s       1.337ms       7.599ms         0.00%      140.005s      65.730ms      15.83 Kb         752 b     216.21 Mb           0 b          2130  \n",
      "                                            aten::copy_         0.71%        2.845s         0.71%        2.845s       1.217ms      140.157s        37.04%      140.157s      59.947ms           0 b           0 b           0 b           0 b          2338  \n",
      "                                              aten::add         0.00%       6.642ms         0.00%       6.642ms      20.692us      91.973ms         0.02%      91.973ms     286.520us           0 b           0 b       5.13 Gb       5.13 Gb           321  \n",
      "                                           aten::conv2d         0.00%      10.024ms         0.36%        1.447s       1.082ms       3.152ms         0.00%        6.380s       4.768ms           0 b           0 b      51.09 Gb           0 b          1338  \n",
      "                                      aten::convolution         0.01%      20.392ms         0.36%        1.453s       1.005ms       3.306ms         0.00%        6.936s       4.797ms           0 b           0 b      58.83 Gb           0 b          1446  \n",
      "                                     aten::_convolution         0.01%      30.075ms         0.36%        1.432s     990.532us       4.739ms         0.00%        6.933s       4.795ms           0 b           0 b      58.83 Gb           0 b          1446  \n",
      "                                aten::cudnn_convolution         0.35%        1.381s         0.35%        1.381s       1.032ms        6.112s         1.62%        6.112s       4.568ms           0 b           0 b      51.09 Gb      51.09 Gb          1338  \n",
      "                                          aten::reshape         0.00%       5.234ms         0.00%       6.300ms      12.831us       1.105ms         0.00%       1.661ms       3.383us           0 b           0 b           0 b           0 b           491  \n",
      "                                   aten::_reshape_alias         0.00%       1.130ms         0.00%       1.130ms       2.274us     569.000us         0.00%     569.000us       1.145us           0 b           0 b           0 b           0 b           497  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 400.190s\n",
      "Self CUDA time total: 378.363s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "        trainer.train_model(trainLoader=wf1TrLoader, validLoader=wf1VaLoader, metric='loss', epochs=epochs)\n",
    "\n",
    "print(prof.key_averages().table(row_limit=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Time: 410.34s\n",
      "Train:  {'rec_loss': 0.08689464412323417, 'd_loss': -0.0663987930506876, 'ec_loss': 0, 'feature_loss': 1008.8556506849316}\n",
      "Validation:  {'rec_loss': 0.053801422688498426, 'd_loss': 0.045036799872099464, 'ec_loss': 8979.912935323384, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 1 Time: 413.67s\n",
      "Train:  {'rec_loss': 0.024357510919440283, 'd_loss': -0.15707351475545805, 'ec_loss': 0, 'feature_loss': 13502.598630136987}\n",
      "Validation:  {'rec_loss': 0.015552279961049853, 'd_loss': -0.012041884275218148, 'ec_loss': 10416.596393034826, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 2 Time: 396.51s\n",
      "Train:  {'rec_loss': 0.009450414082775378, 'd_loss': -0.013519710384003104, 'ec_loss': 0, 'feature_loss': 20635.57397260274}\n",
      "Validation:  {'rec_loss': 0.010691284540280774, 'd_loss': 1.02629709006542e-05, 'ec_loss': 17508.57338308458, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 3 Time: 396.28s\n",
      "Train:  {'rec_loss': 0.006036439007275725, 'd_loss': -5.12942844006705e-06, 'ec_loss': 0, 'feature_loss': 21927.419178082193}\n",
      "Validation:  {'rec_loss': 0.019900262652344965, 'd_loss': 3.7801068563440545e-06, 'ec_loss': 28457.65422885572, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 4 Time: 389.99s\n",
      "Train:  {'rec_loss': 0.0054953228937436455, 'd_loss': -1.8653045615742671e-07, 'ec_loss': 0, 'feature_loss': 21694.205479452055}\n",
      "Validation:  {'rec_loss': 0.0027371885171577114, 'd_loss': 2.9593204216601847e-08, 'ec_loss': 20524.441542288558, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 5 Time: 377.17s\n",
      "Train:  {'rec_loss': 0.004011985700424403, 'd_loss': 4.056473948342139e-07, 'ec_loss': 0, 'feature_loss': 22063.038356164383}\n",
      "Validation:  {'rec_loss': 0.006772758355781214, 'd_loss': -1.2278587520307866e-07, 'ec_loss': 18628.50248756219, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 6 Time: 371.52s\n",
      "Train:  {'rec_loss': 0.003975876063516695, 'd_loss': 4.74751290661117e-07, 'ec_loss': 0, 'feature_loss': 21597.152054794522}\n",
      "Validation:  {'rec_loss': 0.009575537781217206, 'd_loss': -3.4981273113124697e-09, 'ec_loss': 19760.746268656716, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 7 Time: 371.82s\n",
      "Train:  {'rec_loss': 0.003978669806702496, 'd_loss': 5.366284263077869e-07, 'ec_loss': 0, 'feature_loss': 22592.786301369862}\n",
      "Validation:  {'rec_loss': 0.0052204398966547265, 'd_loss': 1.0163605102252178e-07, 'ec_loss': 21117.114427860695, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 8 Time: 379.06s\n",
      "Train:  {'rec_loss': 0.0035866090696151944, 'd_loss': 1.9046045004147782e-07, 'ec_loss': 0, 'feature_loss': 23319.832876712328}\n",
      "Validation:  {'rec_loss': 0.007269952427688523, 'd_loss': 2.750890310457705e-09, 'ec_loss': 21643.45273631841, 'accuracy': 0.0009653226504871502}\n",
      "Epoch 9 Time: 379.95s\n",
      "Train:  {'rec_loss': 0.003557911311110405, 'd_loss': -3.0823795073857047e-07, 'ec_loss': 0, 'feature_loss': 23154.14794520548}\n",
      "Validation:  {'rec_loss': 0.0023780721633588496, 'd_loss': -2.5684136034129072e-09, 'ec_loss': 25445.669154228857, 'accuracy': 0.0009653226504871502}\n"
     ]
    }
   ],
   "source": [
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    torch.jit.onednn_fusion_enabled = True\n",
    "trainer.train_model(trainLoader=wf1TrLoader, validLoader=wf1VaLoader, metric='loss', epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\PIL\\Image.py:3074: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\PIL\\Image.py:3074: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\PIL\\Image.py:3074: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rec_loss': 0.0023254228801262086, 'd_loss': 3.619270367979458e-07, 'ec_loss': 23309.519512195122, 'accuracy': 0.0009458655264319443}\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.evaluate(wf1TsLoader)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For usage with pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_epoch = 9\n",
    "gen_path = pathlib.Path(f'genproject_e{t_epoch}.pt').resolve()\n",
    "dsc_path = pathlib.Path(f'dscproject_e{t_epoch}.pt').resolve()\n",
    "brc_path = pathlib.Path(f'brcproject_e{t_epoch}.pt').resolve()\n",
    "gen = torch.load(gen_path, map_location=device)\n",
    "dsc = torch.load(dsc_path, map_location=device)\n",
    "brc = torch.load(brc_path, map_location=device)\n",
    "pretrain = train.GANProject(load_unet=gen, load_cnn=dsc, load_branch=brc, img_size=128, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf1FireLoader = DataLoader(dataset=wf1fire, batch_size=batch_size)\n",
    "test_result = pretrain.evaluate(wf1TsLoader)\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
