{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import train\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "src = \"\"\n",
    "if os.name == \"nt\":\n",
    "    src = \"D:/599DL4VProject/the_wildfire_dataset\"\n",
    "elif os.name == \"posix\":\n",
    "    src = \"/home/asromelo/Desktop/Projects/599_proj/the_wildfire_dataset/\"\n",
    "\n",
    "wf1TrainPath = pathlib.Path(src + '/train').resolve()\n",
    "wf1ValidPath = pathlib.Path(src + '/valid').resolve()\n",
    "wf1TestPath = pathlib.Path(src + '/test').resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    torch.jit.onednn_fusion_enabled = True\n",
    "\n",
    "# Currently using Kaggle The Wildfire Dataset and FiSmo dataset\n",
    "# https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images\n",
    "# https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# Valid img size is 128, 160, 192, 224\n",
    "img_size = 128\n",
    "\n",
    "tfset = v2.Compose([\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(torch.uint8),\n",
    "        v2.RandomCrop(size=512, pad_if_needed=True, padding_mode='edge'),\n",
    "        v2.RandomChoice(transforms=[\n",
    "            v2.Grayscale(3),\n",
    "            v2.ColorJitter(p,p,p,p),\n",
    "            v2.RandomEqualize(p),\n",
    "            v2.RandomHorizontalFlip(p)\n",
    "        ], p=[0.5,0.3,0.5,0.5]), \n",
    "        v2.Resize(size=img_size, antialias=False),\n",
    "        v2.ConvertImageDtype(dtype=torch.float),\n",
    "])\n",
    "\n",
    "testset = v2.Compose([\n",
    "    v2.ToImageTensor(),\n",
    "    v2.ConvertImageDtype(torch.uint8),\n",
    "    v2.Resize(size=720, interpolation=InterpolationMode.BILINEAR, antialias=True),\n",
    "    v2.CenterCrop(size=512),\n",
    "    v2.Resize(size=img_size, antialias=False),\n",
    "    v2.ConvertImageDtype(dtype=torch.float)\n",
    "])\n",
    "\n",
    "wf1Train = train.ForestFireDataset(root=str(wf1TrainPath), transform=tfset)\n",
    "wf1Valid = train.ForestFireDataset(root=str(wf1ValidPath), transform=testset)\n",
    "wf1Test = train.ForestFireDataset(root=str(wf1TestPath), transform=testset)\n",
    "\n",
    "wf1Tsfire = Subset(wf1Test, [i for i, label in enumerate(wf1Test.targets) if label == 0])\n",
    "wf1Tsnofire = Subset(wf1Test, [i for i, label in enumerate(wf1Test.targets) if label == 1])\n",
    "wf1nofire = Subset(wf1Train, [i for i, label in enumerate(wf1Train.targets) if label == 1])\n",
    "\n",
    "batch_size = 8\n",
    "wf1TrLoader = DataLoader(dataset=wf1nofire, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "wf1VaLoader = DataLoader(dataset=wf1Valid, batch_size=batch_size, num_workers=2)\n",
    "wf1TsLoader = DataLoader(dataset=wf1Test, batch_size=batch_size, num_workers=2)\n",
    "wf1TsFireLd = DataLoader(dataset=wf1Tsfire, batch_size=batch_size, num_workers=2)\n",
    "wf1TsNoFireLd = DataLoader(dataset=wf1Tsnofire, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "trainer = train.GANProject(img_size=img_size, debug=False, small=True, activation=nn.LeakyReLU(0.1), alternative=\"efficientnet_v2_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next W = (W + 2*pad - Kernel)/stride + 1\n",
    "w=128 + 32\n",
    "k=3\n",
    "p=1\n",
    "s=1\n",
    "\n",
    "def downlayer(w,k,p,s):\n",
    "    a1 = conv(w,k,p,s)\n",
    "    a2 = conv(a1,k,p,s)\n",
    "    return conv(a2,2,0,2)\n",
    "\n",
    "def conv(w,k,p,s):\n",
    "    return (w + 2*p - k)/s + 1\n",
    "\n",
    "def unpooled(w,k,p,s):\n",
    "    h = (w + 2*p - k)/s + 1\n",
    "    return (h + 2*p - k)/s + 1\n",
    "\n",
    "def backward(w,k,p,s):\n",
    "    f = 2*(w - 1) + 2 - 2*p\n",
    "    g = s*(f - 1) + k - 2*p\n",
    "    return s*(g - 1) + k - 2*p\n",
    "\n",
    "def uplayer(w,k,p,s):\n",
    "    d = 1\n",
    "    innerkernel = 2\n",
    "    innerstride = 2\n",
    "    innerpad = 0\n",
    "    outpad = 0\n",
    "    # Formula for Conv = (H_in - 1)*stride - 2*p + dilation*(k-1) + outpad + 1\n",
    "    u = innerstride*(w - 1) - 2*innerpad + d*(innerkernel-1) + outpad + 1\n",
    "    a1 = conv(u,k,p,s)\n",
    "    a2 = conv(a1,k,p,s)\n",
    "    return a2\n",
    "    # 148 -> 72 -> 34 -> 30\n",
    "\n",
    "a = downlayer(w,k,p,s)\n",
    "b = downlayer(a,k,p,s)\n",
    "c = downlayer(b,k,p,s)\n",
    "bottom = unpooled(c,k,p,s)\n",
    "d = uplayer(bottom,k,p,s)\n",
    "e = uplayer(d,k,p,s)\n",
    "f = uplayer(e,k,p,s)\n",
    "conv(f,1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Time: 212.20s\n",
      "Train:  {'rec_loss': 0.09379781705630225, 'd_loss': 1.6265230961483663, 'ec_loss': 0, 'feature_loss': 0.771478438449848}\n",
      "Validation:  {'rec_loss': 0.056281678119109044, 'd_loss': 1.6508001071303637, 'ec_loss': 0.003381366160378527, 'accuracy': 8.0}\n",
      "Epoch 1 Time: 219.58s\n",
      "Train:  {'rec_loss': 0.0831479301568585, 'd_loss': 1.6265230961483663, 'ec_loss': 0, 'feature_loss': 1.5029122488839286}\n",
      "Validation:  {'rec_loss': 0.06330721651143696, 'd_loss': 1.6508001071303637, 'ec_loss': 0.003756063494516249, 'accuracy': 8.0}\n",
      "Epoch 2 Time: 213.87s\n",
      "Train:  {'rec_loss': 0.07708621967164941, 'd_loss': 1.6265230961483663, 'ec_loss': 0, 'feature_loss': 2.776107277913659}\n",
      "Validation:  {'rec_loss': 0.08151637025140411, 'd_loss': 1.6508001071303637, 'ec_loss': 0.002462547068572163, 'accuracy': 8.0}\n",
      "Epoch 3 Time: 208.25s\n",
      "Train:  {'rec_loss': 0.07396746116568614, 'd_loss': 1.6265230961483663, 'ec_loss': 0, 'feature_loss': 4.108897107712766}\n",
      "Validation:  {'rec_loss': 0.05758493812523078, 'd_loss': 1.6508001071303637, 'ec_loss': 0.00434694153752493, 'accuracy': 8.0}\n",
      "Epoch 4 Time: 200.54s\n",
      "Train:  {'rec_loss': 0.07214504462245025, 'd_loss': 1.6265230961483663, 'ec_loss': 0, 'feature_loss': 5.579385774838526}\n",
      "Validation:  {'rec_loss': 0.0637625841359001, 'd_loss': 1.6508001071303637, 'ec_loss': 0.016171378282765252, 'accuracy': 8.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model(trainLoader=wf1TrLoader, validLoader=wf1VaLoader, epochs=epochs, eps=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
