{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2065340ccb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import train\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "src = \"\"\n",
    "if os.name == \"nt\":\n",
    "    src = \"D:/599DL4VProject/the_wildfire_dataset\"\n",
    "elif os.name == \"posix\":\n",
    "    src = \"/home/asromelo/Desktop/Projects/599_proj/the_wildfire_dataset/\"\n",
    "\n",
    "wf1TrainPath = pathlib.Path(src + '/train').resolve()\n",
    "wf1ValidPath = pathlib.Path(src + '/valid').resolve()\n",
    "wf1TestPath = pathlib.Path(src + '/test').resolve()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.jit.onednn_fusion_enabled = True\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Currently using Kaggle The Wildfire Dataset and FiSmo dataset\n",
    "# https://www.kaggle.com/datasets/utkarshsaxenadn/landscape-recognition-image-dataset-12k-images\n",
    "# https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  129.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126.0, 62.0, 30.0, 14.0, 11.0, 19.0, 35.0, 67.0, 129.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next W = (W + 2*pad - Kernel)/stride + 1\n",
    "def downlayer(w,k,p,s):\n",
    "    a1 = conv(w,k,p,s)\n",
    "    a2 = conv(a1,k,p,s)\n",
    "    return conv(a2,k,0,2)\n",
    "\n",
    "def conv(w,k,p,s):\n",
    "    return (w + 2*p - k)/s + 1\n",
    "\n",
    "def convBlock(w,ksp:(int,int,int)):\n",
    "    return (w + 2*ksp[2] - ksp[0])/ksp[1] + 1\n",
    "\n",
    "def upBlock(w,ksp:(int,int,int),pad=0):\n",
    "    u = 2*(w-1) - 2*pad + 1*(2-1) + 0 + 1\n",
    "    return convBlock(u,ksp)\n",
    "\n",
    "def unpooled(w,k,p,s):\n",
    "    h = (w + 2*p - k)/s + 1\n",
    "    return (h + 2*p - k)/s + 1\n",
    "\n",
    "def backward(w,k,p,s):\n",
    "    f = 2*(w - 1) + 2 - 2*p\n",
    "    g = s*(f - 1) + k - 2*p\n",
    "    return s*(g - 1) + k - 2*p\n",
    "\n",
    "def uplayer(w,k,p,s,inner=0):\n",
    "    d = 1\n",
    "    innerkernel = 2\n",
    "    innerstride = 2\n",
    "    outpad = 0\n",
    "    # Formula for Conv = (H_in - 1)*stride - 2*p + dilation*(k-1) + outpad + 1\n",
    "    u = innerstride*(w - 1) - 2*inner + d*(innerkernel-1) + outpad + 1\n",
    "    a1 = conv(u,k,p,s)\n",
    "    return a1\n",
    "    # 148 -> 72 -> 34 -> 30\n",
    "\n",
    "def upconv(w,k,p,s):\n",
    "    return s*(w-1) - 2*p + 1*(k-1) + 0 + 1\n",
    "\n",
    "w=129\n",
    "ksp = (4,1,0)\n",
    "\n",
    "e1 = convBlock(w,ksp)\n",
    "e2 = convBlock(e1,(4,2,0))\n",
    "e3 = convBlock(e2,(4,2,0))\n",
    "e4 = convBlock(e3,(4,2,0))\n",
    "bot = convBlock(e4,ksp)\n",
    "d1 = upBlock(bot,ksp)\n",
    "d2 = upBlock(d1,ksp)\n",
    "d3 = upBlock(d2,ksp)\n",
    "d4 = upBlock(d3,ksp,1)\n",
    "print(\"output size: \", conv(d4,1,0,1))\n",
    "img_size = int(d4)\n",
    "(e1,e2,e3,e4,bot,d1,d2,d3,d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "tfset = v2.Compose([\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(torch.uint8),\n",
    "        v2.RandomCrop(size=512, pad_if_needed=True, padding_mode='edge'),\n",
    "        v2.RandomChoice(transforms=[\n",
    "            v2.Grayscale(3),\n",
    "            v2.RandomRotation(45),\n",
    "            v2.RandomEqualize(p),\n",
    "        ], p=[0.3,0.3,0.3]),\n",
    "        v2.RandomHorizontalFlip(p),\n",
    "        v2.Resize(size=img_size, antialias=False),\n",
    "        v2.ConvertImageDtype(dtype=torch.float),\n",
    "])\n",
    "\n",
    "testset = v2.Compose([\n",
    "    v2.ToImageTensor(),\n",
    "    v2.ConvertImageDtype(torch.uint8),\n",
    "    v2.Resize(size=720, interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
    "    v2.CenterCrop(size=512),\n",
    "    v2.Resize(size=img_size, antialias=False),\n",
    "    v2.ConvertImageDtype(dtype=torch.float),\n",
    "])\n",
    "\n",
    "wf1Train = train.ForestFireDataset(root=str(wf1TrainPath), transform=tfset)\n",
    "wf1Valid = train.ForestFireDataset(root=str(wf1ValidPath), transform=testset)\n",
    "wf1Test = train.ForestFireDataset(root=str(wf1TestPath), transform=testset)\n",
    "\n",
    "wf1Tsfire = Subset(wf1Test, [i for i, label in enumerate(wf1Test.targets) if label == 0])\n",
    "wf1Tsnofire = Subset(wf1Test, [i for i, label in enumerate(wf1Test.targets) if label == 1])\n",
    "wf1nofire = Subset(wf1Train, [i for i, label in enumerate(wf1Train.targets) if label == 1])\n",
    "\n",
    "batch_size = 16\n",
    "wf1TrLoader = DataLoader(dataset=wf1nofire, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "wf1VaLoader = DataLoader(dataset=wf1Valid, batch_size=batch_size, num_workers=2)\n",
    "wf1TsLoader = DataLoader(dataset=wf1Test, batch_size=batch_size, num_workers=2)\n",
    "wf1TsFireLd = DataLoader(dataset=wf1Tsfire, batch_size=batch_size, num_workers=2)\n",
    "wf1TsNoFireLd = DataLoader(dataset=wf1Tsnofire, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "trainer = train.GANProject(img_size=img_size, debug=False, small=True, activation=nn.LeakyReLU(0.2), alternative=\"shufflenet_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Narisa\\Dropbox\\studyDotLib\\Fall2023\\CPSC599\\Project\\train.py:128: UserWarning: Using a target size (torch.Size([16, 3, 129, 129])) that is different to the input size (torch.Size([16, 3, 114, 114])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  rec_loss = F.l1_loss(img_fake, img_real)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (114) must match the size of tensor b (129) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Narisa\\Dropbox\\studyDotLib\\Fall2023\\CPSC599\\Project\\AutoStart.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Narisa/Dropbox/studyDotLib/Fall2023/CPSC599/Project/AutoStart.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_model(trainLoader\u001b[39m=\u001b[39;49mwf1TrLoader, validLoader\u001b[39m=\u001b[39;49mwf1VaLoader, metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs\u001b[39m=\u001b[39;49mepochs, eps\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Narisa\\Dropbox\\studyDotLib\\Fall2023\\CPSC599\\Project\\train.py:83\u001b[0m, in \u001b[0;36mGANProject.train_model\u001b[1;34m(self, trainLoader, validLoader, metric, epochs, previous, eps)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     82\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 83\u001b[0m     t_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mganTrain_epoch(trainLoader)\n\u001b[0;32m     84\u001b[0m     v_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(validLoader)\n\u001b[0;32m     85\u001b[0m     delta_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Narisa\\Dropbox\\studyDotLib\\Fall2023\\CPSC599\\Project\\train.py:128\u001b[0m, in \u001b[0;36mGANProject.ganTrain_epoch\u001b[1;34m(self, trainLoader)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39m# Image Reconstruction Loss\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 128\u001b[0m rec_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49ml1_loss(img_fake, img_real)\n\u001b[0;32m    129\u001b[0m rec_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_Gen\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3263\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3260\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3261\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3263\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3264\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (114) must match the size of tensor b (129) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "trainer.train_model(trainLoader=wf1TrLoader, validLoader=wf1VaLoader, metric='loss', epochs=epochs, eps=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
